{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is an unsupervised machine learning technique that abstract topics from a collection of documents. This technique can be used for multiple applications such as document clustering, recommendation systems and more.\n",
    "\n",
    "In this tutorial we will examine two ways to do topic modeling LDA and LSA.\n",
    "\n",
    "## Summary\n",
    "- Latent Dirichlet Allocation (LDA) \n",
    "- Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data includes 20 groups of topics, we will select few to experiment with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ 'comp.graphics', 'rec.motorcycles', 'talk.politics.guns', 'comp.windows.x',\n",
    "              'misc.forsale', 'rec.sport.baseball']\n",
    "data = fetch_20newsgroups(categories=categories, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3503, 3503)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.data), len(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This past week I've been playing with some of the R-D (Reaction-\n",
      "Diffusion, not to be confused with RDS or R&D) techniques\n",
      "from SIGGRAPH '91.\n",
      "\n",
      "I was wondering what material is available to explain the control\n",
      "mechanism a little more.  It seems to me very much like a matter of\n",
      "picking random magic numbers and sitting back and waiting.  Although\n",
      "both of the papers (Turk and Witkin & Kass) were very well organized\n",
      "and extremely helpful, I guess what I need is a more basic description\n",
      "of the technique, especially wrt the control mechanisms.  The tests\n",
      "that I did had a tendency to either turn into blurry mud or become\n",
      "unstable.\n",
      "\n",
      "Is there any info available online?  Source code would be great but\n",
      "not necessary.\n",
      "\n",
      "Thanks!\n",
      "\n",
      "\n",
      "-- \n"
     ]
    }
   ],
   "source": [
    "print(data.data[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "my_limitizer = WordNetLemmatizer()\n",
    "\n",
    "def limmitizer(text):\n",
    "    \n",
    "    words = str(text).strip().lower()\n",
    "    words = re.sub(r'[\\W\\d\\s_]',' ',words)\n",
    "    words = words.split()\n",
    "    \n",
    "    words = [my_limitizer.lemmatize(word,pos='v') for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data.data)):\n",
    "    data.data[i]=limmitizer(data.data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no rear tire as small as there be some front though so get a instead be there anything that size any other recomendations call the tire company yourself and tell them what you have they can make recomendations for you that s your best bet check a biker magazine cycle world etc for phone number it s possible there be no other tire available though erik astrup afm dod cbr rr cbr concours ninja'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's vectorize the data to be able to use it in the `LDA` algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our data to a train and test split to test our model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, test_size=.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2802, 3000), (701, 3000))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_v = vectorizer.fit_transform(x_train)\n",
    "x_test_v = vectorizer.transform(x_test)\n",
    "\n",
    "x_train_v.shape,x_test_v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notice that 2802 + 701 = 3503 which is the nomber of len(data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LDA\n",
    "\n",
    "### **LDA** is an unsupervised algorithm, like the **PCA** it doesn't accept a **`y`** aka a target variable. **LDA** learns to model the topics on tokens/words basis, so that each word is assigned a probability that it belong to a specific topic, thus one document can belong to multiple topics, and the topic with the highest probability is assigned to the document.\n",
    "\n",
    "### Because **LDA** is an unsupervised model, it doesn't know the topics, it create a form of cluster of words, and this cluster is then called a topic, and it's up to you to name the cluster base on your interpretation.\n",
    "\n",
    "### **LDA** accepts a number of components in it's constructor, the number of components is the number of topics you would like your algorithm to detect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=6, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=6 , random_state=42)\n",
    "lda.fit(x_train_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how topics are represented in the fitted **`lda`**\n",
    "\n",
    "Recall we created a `vectorizer` with `3000` words, the `lda` will assign a probability for each word that it belongs to this specific topic, let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3000)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.09159186, 3.59344538, 1.95607715, ..., 0.65334361, 0.16693612,\n",
       "        0.16685356],\n",
       "       [0.16671551, 0.16668502, 0.16670557, ..., 0.16707109, 0.16671534,\n",
       "        0.16668182],\n",
       "       [0.80870455, 0.1676063 , 0.16690144, ..., 0.16667477, 1.97343481,\n",
       "        0.16683954],\n",
       "       [0.16684762, 0.16763242, 0.56104622, ..., 0.16897255, 0.16667643,\n",
       "        0.16673571],\n",
       "       [0.16817669, 0.16721707, 0.16726299, ..., 0.16738263, 0.88995903,\n",
       "        0.16676554],\n",
       "       [0.65260596, 0.42788096, 0.17012023, ..., 1.71382162, 0.16673981,\n",
       "        3.83271304]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'school'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[2305]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2944436873757983"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_[0][2305]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first topic for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 248 1621 1441  200 2311 2297  466  181 1060 2758]\n",
      "bell 1.5401075591544078\n",
      "middle 1.606142042762906\n",
      "larger 1.4476786251711273\n",
      "aware 0.9990476233459842\n",
      "score 5.4230079810009295\n",
      "saw 5.061445679476285\n",
      "cobra 0.16806091181706867\n",
      "audio 0.16965489594476243\n",
      "game 18.326401542778616\n",
      "understand 7.531211076219876\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.randint(0, 3000, size=(10))\n",
    "print(rng)\n",
    "for word, prob in zip(np.array(vectorizer.get_feature_names())[rng], lda.components_[0, rng]):\n",
    "    print(word, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract the features or the set of tokens for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  array([1.09159186, 3.59344538, 1.95607715, ..., 0.65334361, 0.16693612,\n",
       "         0.16685356])),\n",
       " (1,\n",
       "  array([0.16671551, 0.16668502, 0.16670557, ..., 0.16707109, 0.16671534,\n",
       "         0.16668182])),\n",
       " (2,\n",
       "  array([0.80870455, 0.1676063 , 0.16690144, ..., 0.16667477, 1.97343481,\n",
       "         0.16683954])),\n",
       " (3,\n",
       "  array([0.16684762, 0.16763242, 0.56104622, ..., 0.16897255, 0.16667643,\n",
       "         0.16673571])),\n",
       " (4,\n",
       "  array([0.16817669, 0.16721707, 0.16726299, ..., 0.16738263, 0.88995903,\n",
       "         0.16676554])),\n",
       " (5,\n",
       "  array([0.65260596, 0.42788096, 0.17012023, ..., 1.71382162, 0.16673981,\n",
       "         3.83271304]))]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(lda.components_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([15, 8, 8, 5, 6, 9, 2], [1, 7])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [15, 8 , 8 , 5 , 6 , 9 , 2 , 4 , 7 ,1 ]\n",
    "l[:-3],l[:-3:-1] # -1 means move from right to left (e.g. reverse the order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {}\n",
    "names = vectorizer.get_feature_names()\n",
    "\n",
    "for idx, topic in enumerate(lda.components_):  #6 components * 3000 words\n",
    "    features = topic.argsort()[:-(12-1): -1]   #return indices of last 8 lda weights (e.g. the biggest 8)\n",
    "    #print(features)\n",
    "    tokens = [names[i] for i in features]\n",
    "    topics[idx] = tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['francisco', 'tapes', 'outside', 'death', 'intended', 'limited', 'road', 'ya', 'jackson', 'landon']\n",
      "['2b', 'sabretooth', 'finding', 'killed', 'behavior', 'production', 'technology', 'independent', 'sphere', 'equivalent']\n",
      "['rides', 'needs', 'sales', 'section', 'center', 'monday', 'hi', 'points', 'defend', '85']\n",
      "['ascii', '2nd', 'added', 'detail_win', 'thanx', 'bo', '98', 'built', 'buying', 'usage']\n",
      "['undefined', 'talent', 'exception', 'posting', 'whitespace', 'jackson', 'force', 'wide', 'buyer', 'lies']\n",
      "['ansi', 'remove', 'pascal', 'field', 'dealer', 'road', 'giants', 'tapes', 'defend', 'intended']\n"
     ]
    }
   ],
   "source": [
    "for key in topics:\n",
    "    print(topics[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check back our topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp.graphics',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'talk.politics.guns']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52098168, 0.02191683, 0.06943509, 0.02246308, 0.33997713,\n",
       "        0.02522618]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(x_test_v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65107061, 0.06955011, 0.07004151, 0.06953725, 0.06982984,\n",
       "        0.06997068]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(vectorizer.transform(['i want to play basketball']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0703407 , 0.06935892, 0.06989191, 0.06935884, 0.6508159 ,\n",
       "        0.07023373]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(vectorizer.transform(['windows is good']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65063547, 0.0698604 , 0.06985681, 0.06985998, 0.06993068,\n",
       "        0.06985667]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(vectorizer.transform(['wars are bad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def model_text(index):\n",
    "    print(f\"the text to test is:\\n {x_test[index]}\\n\")\n",
    "    print(f\"this text belogs to {data.target_names[y_test[index]]} class\\n\")\n",
    "    probs = lda.transform(x_test_v[index])\n",
    "    print(\"topic probs for the input:\\n\")\n",
    "    for i, prob in enumerate(probs.flatten()):\n",
    "        print(f\"topic {topics[i]} prob {round(prob, 3)}\")\n",
    "    print(f\"the highest topic prob is {topics[np.argmax(probs)]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the text to test is:\n",
      " There is a multi threaded xlib version written.\n",
      "Do an archie search for mt-xlib:\n",
      "Host export.lcs.mit.edu\n",
      "\n",
      "    Location: /contrib\n",
      "      DIRECTORY drwxr-xr-x        512  Jul 30 1992  mt-xlib\n",
      "    Location: /contrib/mt-xlib-1.1\n",
      "           FILE -rw-r--r--     106235  Jan 21 14:02  mt-xlib-xhib92.ps.Z\n",
      "           FILE -rw-r--r--    1658123  Jan 21 14:03  mt-xlib.tar.Z\n",
      "    Location: /contrib/mt-xlib\n",
      "           FILE -rw-r--r--     106235  Jul 30 1992  mt-xlib-xhib92.ps.Z\n",
      "           FILE -rw-r--r--    1925529  Jul 30 1992  mt-xlib.tar.Z\n",
      "\n",
      "this text belogs to comp.windows.x class\n",
      "\n",
      "topic probs for the input:\n",
      "\n",
      "topic ['francisco', 'tapes', 'outside', 'death', 'intended', 'limited', 'road', 'ya', 'jackson', 'landon'] prob 0.345\n",
      "topic ['2b', 'sabretooth', 'finding', 'killed', 'behavior', 'production', 'technology', 'independent', 'sphere', 'equivalent'] prob 0.035\n",
      "topic ['rides', 'needs', 'sales', 'section', 'center', 'monday', 'hi', 'points', 'defend', '85'] prob 0.036\n",
      "topic ['ascii', '2nd', 'added', 'detail_win', 'thanx', 'bo', '98', 'built', 'buying', 'usage'] prob 0.035\n",
      "topic ['undefined', 'talent', 'exception', 'posting', 'whitespace', 'jackson', 'force', 'wide', 'buyer', 'lies'] prob 0.409\n",
      "topic ['ansi', 'remove', 'pascal', 'field', 'dealer', 'road', 'giants', 'tapes', 'defend', 'intended'] prob 0.14\n",
      "the highest topic prob is ['undefined', 'talent', 'exception', 'posting', 'whitespace', 'jackson', 'force', 'wide', 'buyer', 'lies']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_text(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example **`LDA`** detect the topic of the input as the same topic that has `bike` in it.\n",
    "\n",
    "Let's investigate the `lda` object output and how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.motorcycles'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names[y_test[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0373075 , 0.03682845, 0.03716172, 0.03703984, 0.81491558,\n",
       "        0.0367469 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = lda.transform(x_test_v[1])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output of the transform function is a numpy array that contains the probability that this text belongs to each of the topics, for instance here we can tell that that particular input belong to the first topic by a `0.037` probability, and the second one by `0.036` probability and so on.\n",
    "\n",
    "So we get the `argmax` of the probabilities which is the index of the maximum probability, that would be the topic group to which this text belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and you can recall the topic from the `topics` dictionary we have declared before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thanks', 'use', 'window', 'sale', 'know', 'mail', '00', 'email']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA)\n",
    "\n",
    "As we saw in the **LDA** we model the topics by assigning each word to a topic and measure how much of a certain topic is presented in the document and thus assign the document to this topic.\n",
    "\n",
    "**LSA** works in a pretty different way, first we create a term-doc matrix where each word is represented by a row and each document in a column, then applying **SVD** on this matrix will produce three matrices:\n",
    "\n",
    "1. term topic matrix **`u`** .\n",
    "2. topic importance matrix **`s`**.\n",
    "3. topic document matrix **`vh`**.\n",
    "\n",
    "So basically **LSA** is like **PCA** but for text documents, we reduce the features which in our case the words to topics which capture the variance of the words thus the topic of them.\n",
    "\n",
    "To be able to apply **SVD** on sparse matrices (the matrices that are returned by the text vectorizers of sklearn) we need to use `TruncatedSVD` class from sklearn as it doesn't center the data around zero like `PCA` does, thus it can work with the sparse matrices that we have.\n",
    "\n",
    "Let's see how can we apply **LSA** using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-99d3e1c3cb0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# we will reuse the vectors we have created before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train_v' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# we will reuse the vectors we have created before\n",
    "lsa = TruncatedSVD(n_components=6, random_state=42)\n",
    "lsa.fit(x_train_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that the number of components is used to calculate the singular values, specifying it to 6 means that we only wants to calculate 6 singular values aka 6 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3000)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the topics we have with **LSA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_lsa = {}\n",
    "names = vectorizer.get_feature_names()\n",
    "\n",
    "for idx, topic in enumerate(lsa.components_):\n",
    "    features = topic.argsort()[:-(10-1): -1]\n",
    "    tokens = [names[i] for i in features]\n",
    "    topics_lsa[idx] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use', 'know', 'like', 'don', 'think', 'just', 'make', 'thank']\n",
      "['file', 'thank', 'program', 'use', 'window', 'windows', 'color', 'mail']\n",
      "['offer', 'sale', 'ship', 'sell', 'new', 'drive', 'condition', 'game']\n",
      "['game', 'team', 'win', 'run', 'pitch', 'year', 'hit', 'score']\n",
      "['bike', 'ride', 'motorcycle', 'thank', 'know', 'dod', 'look', 'dog']\n",
      "['thank', 'mail', 'know', 'list', 'send', 'advance', 'post', 'address']\n"
     ]
    }
   ],
   "source": [
    "for key in topics_lsa:\n",
    "    print(topics_lsa[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this with the **LDA** topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gun', 'think', 'people', 'don', 'just', 'make', 'say', 'year', 'know', 'like']\n",
      "['allocate', 'sega', 'genesis', 'lethal', 'cells', 'radiosity', 'ti', 'japan', 'steve', 'favorite']\n",
      "['sale', 'offer', 'sell', 'ship', 'condition', 'new', 'include', 'price', 'drive', 'ask']\n",
      "['brave', 'alomar', 'baerga', 'edu', 'tony', 'chop', 'atlanta', 'cnn', 'colorado', 'vacation']\n",
      "['use', 'thank', 'file', 'program', 'window', 'know', 'graphics', 'windows', 'color', 'mail']\n",
      "['bike', 'ride', 'pitch', 'game', 'dog', 'say', 'helmet', 'think', 'drive', 'just']\n"
     ]
    }
   ],
   "source": [
    "for key in topics:\n",
    "    print(topics[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each algorithm will have his own topics, and we can test them to see which one fits best in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_text_lsa(index):\n",
    "    print(f\"the text to test is:\\n {x_test[index]}\\n\")\n",
    "    print(f\"this text belogs to {data.target_names[y_test[index]]} class\\n\")\n",
    "    probs = lsa.transform(x_test_v[index])\n",
    "    print(\"topic probs for the input:\")\n",
    "    for i, prob in enumerate(probs.flatten()):\n",
    "        print(f\"topic {topics_lsa[i]} prob {round(prob, 3)}\")\n",
    "    print(f\"the highest topic prob is {topics_lsa[np.argmax(probs)]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the text to test is:\n",
      " no he s not nut wip be second to none the sport station they don t have tony bruno work espn radio and al morganti do friday night hockey because they suck i live in richmond va but i visit phila often and on the way i get wtem washington and wip i hear the fan at night wherever i go the signal use to be wnbc when they play golden oldies because you can t avoid it of those three wip have the best host hand down chuck cooperstein isn t a homer and neither be jody mac wtem be too generic to be place in the catergory in fact if you have hear wtem and the fan you notice the theme music be identical same ownership i think so wip be totally original their host actually have a personality this be a knock at tem the team not the fan because mike and the mad dog and sommers be good i mean compare the morning guy in philadelphia to the ones in washington be a total joke anyway i like the fan and wip but i think the edge go to ip when i get back from philly i go into withdraw cause richmond have nada except the national sport line and those guy be totally clueless i be really mad when wcau be cancel because they have steve fredericks do sport phone after the phillies game wcau be another strong station now it s an oldies station but they still have the phillies i start listen to the fan because i hear he go there i finally hear him last summer and he wasn t the same guy those ny fan get to him i be glad to hear him back in philly when i go to see a few eagle game i will admit i be die hard eagle fan and wip be basically an eagle station days a year but i bet you the phillies be in control right now about the knock on g cobb i like him he know the eagle like a book i remember the weekend before they go to play san fran when everyone think the eagle would be blow away cobb say that the eagle usually play their best when no one believe they can win well they be inch shy of pull the victory\n",
      "\n",
      "this text belogs to rec.sport.baseball class\n",
      "\n",
      "topic probs for the input:\n",
      "topic ['use', 'know', 'like', 'don', 'think', 'just', 'make', 'thank'] prob 0.181\n",
      "topic ['file', 'thank', 'program', 'use', 'window', 'windows', 'color', 'mail'] prob -0.13\n",
      "topic ['offer', 'sale', 'ship', 'sell', 'new', 'drive', 'condition', 'game'] prob 0.024\n",
      "topic ['game', 'team', 'win', 'run', 'pitch', 'year', 'hit', 'score'] prob 0.098\n",
      "topic ['bike', 'ride', 'motorcycle', 'thank', 'know', 'dod', 'look', 'dog'] prob -0.001\n",
      "topic ['thank', 'mail', 'know', 'list', 'send', 'advance', 'post', 'address'] prob 0.004\n",
      "the highest topic prob is ['use', 'know', 'like', 'don', 'think', 'just', 'make', 'thank']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_text_lsa(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how you use the **LSA** in sklearn.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "### **LDA** and **LSA** are topic modeling algorithms that uses dimensionality reduction techniques to model the topics in the corpus.\n",
    "# they can be used in Medium or Quora for example to recommend to the reader other similar articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
